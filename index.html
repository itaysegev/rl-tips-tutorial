<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>RL Tips and Tricks</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/white.css" id="theme">
		<!-- Add DLR logo -->
		<!-- <link rel="stylesheet" href="css/dlr.css"> -->
		<!-- Grid system: http://flexboxgrid.com/ -->
		<link rel="stylesheet" href="css/flexboxgrid.min.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css" id="highlight-theme">
		<!-- <link rel="stylesheet" href="plugin/highlight/monokai-sublime.css" id="highlight-theme"> -->
		<!-- <link rel="stylesheet" href="plugin/highlight/atom-one-dark.css" id="highlight-theme"> -->
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section>
					<!-- <h1 class="r-fit-text">RL Tips and Tricks</h1>
					<h3>DLR Template</h3> -->
					<div class="row bottom-xs">
						<div class="row middle-xs">
							<div class="col-xs-7">
								<div class="col-xs-12">
									<h3 id='main-title'>RL Tips and Tricks</h3>
								</div>
							</div>
						</div>
					</div>

				</section>
				<section>
					<h3>Outline</h3>
					<ol>
						<li>
							RL Tips and Tricks
							<ol class="small-text">
								<li>General Nuts and Bolts of RL Experimentation</li>
								<li>RL in practice on a custom task</li>
							</ol>
						</li>
						<li>
							Practical Tips for Reliable RL
							<ol class="small-text">
								<li>Best Practices for Empirical RL</li>
								<li>SB3: Reliable RL Implementations</li>
								<li>RL Zoo: Reproducible Experiments</li>
								<li>Implementing a New Algorithm</li>

							</ol>
						</li>

					</ol>
				</section>
				<section>
					<h2>RL Tips And Tricks</h2>
				</section>
				<section> 
					<h3>1. General Nuts and Bolt of RL Experimentation</h3>
				</section>
				<section>
					<h3>RL is Hard (1/2)</h3>
					<div class="row">
						<div class="col-xs-6">
							<img src="images/a2c.png" alt="A2C" style="max-width: 100%">
							<p class="xsmall-text caption">Which algorithm is better?</p>
						</div>
						<div class="col-xs-6">
							<p class="medium-text fragment">
								The only difference: the epsilon value to avoid division by zero in the optimizer
								(one is <code class="medium-text">eps=1e-7</code>
								the other <code class="medium-text">eps=1e-5</code>)
							</p>
						</div>
					</div>
					<aside class="notes">
						A and B are actually the same RL algorithm (A2C),
						sharing the exact same code, same hardware, same hyperparameters...
						except the epsilon value to avoid division by zero in the optimizer
					</aside>

				</section>
				<section>
					<h3>RL is Hard (2/2)</h3>
					<ul>
						<li class="fragment">Data collection by the agent itself</li>
						<li class="fragment">Sensitivity to the random seed / hyperparameters</li>
						<li class="fragment">Sample inefficient</li>
						<li class="fragment">Reward function design</li>
					</ul>
					<p class="xsmall-text fragment">
						<a href="https://www.alexirpan.com/2018/02/14/rl-hard.html">RL is hard blog post</a>,
						<a href="https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html">RL tips and tricks</a>
					</p>
					<aside class="notes">
						- RL is hard (because quite different from supervised learning, data collection on the policy which depends on the data)
					</aside>
				</section>
				<section>
					<div class="row">
						<div class="col-xs-12">
							<img src="images/rl_glitch.png" alt="RL Glitch" style="max-width: 40%">
						</div>
					</div>
					<p class="xsmall-text">Credits: Rishabh Mehrotra (@erishabh)</p>
					<aside class="notes">
						reward hacking, if it can maximises its reward without
						solving the task, it will do it!
					</aside>
				</section>
				<section>
					<h3>Best Practices</h3>
					<ul class="medium-text">
						<li class="fragment">Quantitative evaluation</li>
						<li class="fragment">Use recommended hyperparameters</li>
						<li class="fragment">Save all experiments parameters</li>
						<li class="fragment">
							Use the
							<a href="https://github.com/DLR-RM/rl-baselines3-zoo">RL zoo</a>
						</li>
					</ul>
					<p class="xsmall-text fragment">
						<a href="https://arxiv.org/abs/1709.06560">
							Deep RL that matters (Henderson et al.)
						</a>

					</p>
					<aside class="notes">
						- best practices for comparison: separate eval,
							quantitative results, tune hyperparameters <br>
						- save all parameters
							to reproduce the run (all included in the RL Zoo) <br>
						- rl zoo only when you know what you are doing <br>
						- there is no silver bullet <br>
						- more on what to do when it doesn't work later
					</aside>
				</section>

				<section>
					<div style="position:relative;">
						<h3 class="fragment fade-in-then-out" data-fragment-index="1" style="position:absolute; margin-left: auto; margin-right: auto; left: 0; right: 0;" >
							RL in practice on a custom task
						</h3>
						<h3 class="fragment fade-in-then-out" data-fragment-index="2" style="position:absolute; margin-left: auto; margin-right: auto; left: 0; right: 0;" >
							Do you need RL?
						</h3>
						<h3 class="fragment" data-fragment-index="3" style="position:absolute; margin-left: auto; margin-right: auto; left: 0; right: 0;" >
							Do you really need RL?
						</h3>
					</div>

					<div class="row" style="margin-top: 50px;">
						<div class="col-xs-12">
							<img class="fragment" data-fragment-index="3" src="images/mr_bean_sure.jpg" alt="Mr Bean Are you sure meme"/>
						</div>
					</div>

					<aside class="notes">
						- Do I really need RL? If PID does the job, then don't, unless for education <br>

						Most of the advices are already in the SB3 documentation.
					</aside>
				</section>
				<section>
					<h3>Defining a custom task</h3>
					<ul>
						<li class="fragment">Observation space</li>
						<li class="fragment">Action space</li>
						<li class="fragment">Reward function</li>
						<li class="fragment">Termination conditions</li>
					</ul>
					<aside class="notes">
						Always start simple!
					</aside>
				</section>
				<section>
					<h3>Choosing the observation space</h3>
					<ul>
						<li class="fragment">Enough information to solve the task</li>
						<li class="fragment">Do not break Markov assumption</li>
						<li class="fragment">Normalize!</li>
					</ul>
					<aside class="notes">
						normalize especially for PPO/A2C + running average when you don't know the limits in
						advance (VecNormalize) <br>
					</aside>
				</section>

				<section>
					<h3>Choosing the Action space</h3>
					<ul>
						<li class="fragment">Discrete / Continuous</li>
						<li class="fragment">Complexity vs final performance</li>
					</ul>
					<aside class="notes">
						depends on your task, sometimes you don't have the choice (e.g. atari games)
						for robotics, makes more sense to use continuous action <br>
						bigger action space: better performance at the end but may take much longer to train
						(example: racing car) <br>
						+ trial and errors
					</aside>
				</section>

				<section>
					<h3>Continuous action space: Normalize? Normalize!</h3>
					<div class="row">
						<div class="col-xs-12 medium-text r-stack">
							<pre class="fragment"><code data-trim data-line-numbers="1-6|7-9|11-13|15-19" class="python">
							from gym import spaces

							# Unnormalized action spaces only work with algorithms
							# that don't directly rely on a Gaussian distribution to define the policy
							# (e.g. DDPG or SAC, where their output is rescaled to fit the action space limits)

							# LIMITS TOO BIG: in that case, the sampled actions will only have values
							# around zero, far away from the limits of the space
							action_space = spaces.Box(low=-1000, high=1000, shape=(n_actions,), dtype="float32")

							# LIMITS TOO SMALL: in that case, the sampled actions will almost
							# always saturate (be greater than the limits)
							action_space = spaces.Box(low=-0.02, high=0.02, shape=(n_actions,), dtype="float32")

							# BEST PRACTICE: action space is normalized, symmetric
							# and has an interval range of two,
							# which is usually the same magnitude as the initial standard deviation
							# of the Gaussian used to sample actions (unit initial std in SB3)
							action_space = spaces.Box(low=-1, high=1, shape=(n_actions,), dtype="float32")
							</code></pre>

							<img src="images/gaussian.png" alt="Gaussian" class="fragment" style="max-width: 100%">

						</div>
					</div>
					<aside class="notes">
						- Common pitfalls: observation normalization, action space normalization (ex continuous action) -> use the env checker

					</aside>
				</section>
				<section>
					<h3>Choosing the reward function</h3>
					<ul>
						<li class="fragment">Start with reward shaping</li>
						<li class="fragment">Primary / Secondary reward</li>
						<li class="fragment">Normalize!</li>
					</ul>
					<aside class="notes">
						- reward shaping: careful with reward hacking<br>
						- choosing weights for rewards: primary and secondary
						 look at the magnitude (ex continuity too high, it will do nothing)
					</aside>
				</section>
				<section>
					<h3>Termination conditions?</h3>
					<ul>
						<li class="fragment">Early stopping</li>
						<li class="fragment">Special treatment needed for timeouts</li>
						<li class="fragment">Should not change the task (reward hacking)</li>
					</ul>
					<aside class="notes">
						- early stopping: prevent the agent to explore useless regions of your env
						make learning faster <br>
						- careful or reward hacking: if you penalize at every steps but
						stop the episode early if it explores unwanted regions:
						will maximise its reward by stopping the episode early
					</aside>
				</section>
				<section>
					<h3>Which algorithm to choose?</h3>
					<div class="row">
						<div class="col-xs-12">
							<img src="images/algo_flow_dark.png" alt="Algo flow" style="max-width: 80%">
						</div>
					</div>
					<aside class="notes">
						- Which algorithm should I use? depends on the env and on what matters for you
						action space + multiprocessing (wall clock time vs sample efficiency)? <br>
						- w.r.t. performance: usually a hyperparameter problem (between the latest algo)
						for continuous control: use TQC <br>
						- even more parallel: ES (cf previous lecture)
					</aside>
				</section>
				<section>
					<h3>It doesn't work!</h3>
					<ul class="medium-text">
						<li class="fragment">Did you follow the best practices?</li>
						<li class="fragment">Start simple</li>
						<li class="fragment">Use trusted implementations</li>
						<li class="fragment">Increase budget</li>
						<li class="fragment">Hyperparameter tuning (<a href="https://github.com/optuna/optuna">Optuna</a>)</li>
					</ul>

					<aside class="notes">
						- What to do when it does not work? <br>
							First, use trusted implementation (SB3) with recommend hyperparameters
							(cf papers and RL zoo, ex normalization for PPO)<br>
							then try with more budget and with hyperparameters tuning before saying "it does not work"
							<br>
							Iterate quickly
					</aside>
				</section>
				<section>
					<h4>Best Practices for Empirical RL</h4>
					<div class="row">
						<div class="col-xs-12">
							<ul class="medium-text">
								<li class="fragment">
									<a href="https://arxiv.org/abs/2304.01315">
										Empirical Design in Reinforcement Learning
									</a>
								</li>
								<li class="fragment">
									<a href="https://araffin.github.io/post/rliable/">
										Rliable: Better Evaluation for Reinforcement Learning
									</a>
								</li>
							</ul>
						</div>

						<div class="col-xs-12 fragment">
							<img class="shadow" src="images/IQM.jpeg" style="max-width:60%" alt="">
						</div>

					</div>
					<aside class="notes">
						Recommended resources for going further,
						how to do better science (not only RL),
						better evaluations, CI and etc.
					</aside>
				</section>

				<section>
					<h3>Why Do We Need Better Metrics?</h3>
					<div class="row">
						<div class="col-xs-6">
							<ul class="medium-text">
								<li class="fragment">High variance in RL results</li>
								<li class="fragment">Few training runs (3-10 typical)</li>
								<li class="fragment">Point estimates can be misleading</li>
								<li class="fragment">Different evaluation protocols</li>
							</ul>
						</div>
						<div class="col-xs-6 fragment">
							<img src="images/runs_over_time.png" style="max-width:90%" alt="Number of runs over time">
							<!-- Place Figure 1 from paper showing decreasing number of runs -->
						</div>
					</div>
				</section>

				<section>
					<h4>Reproducibility Findings</h4>
					<img src="images/reported.svg" alt="">
					<p class="small-text">
						Agarwal, Rishabh, et al. "Deep reinforcement learning at the edge of the statistical precipice." Neurips (2021)
					</p>
				</section>

				<section>
					<h3>Performance Profiles</h3>
					<div class="row">
						<div class="col-xs-6">
							<ul class="medium-text">
								<li class="fragment">Show complete performance distribution</li>
								<li class="fragment">More robust to outliers</li>
								<li class="fragment">Better than tables of means</li>
							</ul>
						</div>
						<div class="col-xs-6 fragment">
							<img src="images/score_distributions.png" style="max-width:90%" alt="Score distributions">
							<!-- Place Figure 7 from paper showing score distributions -->
						</div>
					</div>
				</section>

				<section>
					<h3>Advanced Evaluation Metrics</h3>
					<div class="row">
						<div class="col-xs-6">
							<ul class="medium-text">
								<li class="fragment" data-fragment-index="1">
									<strong>Interquartile Mean (IQM)</strong>
									<ul>
										<li>Middle 50% of runs</li>
										<li>More statistically efficient than median</li>
									</ul>
								</li>
								<li class="fragment" data-fragment-index="2">
									<strong>Optimality Gap</strong>
									<ul>
										<li>Distance from optimal performance</li>
										<li>Robust alternative to mean</li>
									</ul>
								</li>
								<li class="fragment" data-fragment-index="3">
									<strong>Stratified Bootstrap CIs</strong>
									<ul>
										<li>Resampling with replacement</li>
										<li>Works well with few runs (5-10)</li>
									</ul>
								</li>
							</ul>
						</div>
						<div class="col-xs-6">
							<img class="fragment" data-fragment-index="1" 
								 src="images/iqm_viz.png" 
								 style="max-width:90%; position: absolute;" 
								 alt="IQM visualization"> 
							<img class="fragment" data-fragment-index="2" 
								 src="images/bootstrap_ci.png" 
								 style="max-width:90%; position: absolute" 
								 alt="Bootstrap CI example">
						</div>
					</div>
					<aside class="notes">
						- IQM: Explain how it provides better statistical efficiency while remaining robust
						- Optimality Gap: Emphasize how it helps measure room for improvement
						- Bootstrap CIs: Highlight importance of uncertainty estimation with few runs
						- The images will stack/replace each other as you progress through fragments
					</aside>
				</section>

				<section>
					<section>
						<h4>Stable-Baselines3</h4>
						<div class="row">
							<div class="col-xs-12">
								Reliable RL Implementations
							</div>

						</div>
						<div class="row">
							<div class="col-xs-4">
								<img src="images/sb3/sb_logo.png" class="shadow" alt="" style="max-width:100%">
							</div>
							<div class="col-xs-8">
								<img src="images/sb3/sb3_train.jpeg" alt="" style="max-width:80%">
							</div>
						</div>

						<p class="medium-text">
							<a href="https://github.com/DLR-RM/stable-baselines3">https://github.com/DLR-RM/stable-baselines3</a>
						</p>

						<aside class="notes">
							What it is? Why is it there?
						</aside>
					</section>
					<section>
						<h4>Reliable Implementations?</h4>

						<img src="images/sb3/all_green.png" style="max-width: 50%" alt="">

						<ul class="medium-text">
							<li class="fragment">Performance checked</li>
							<li class="fragment">Software best practices (96% code coverage, type checked, ...)</li>
							<li class="fragment">3 types of tests (run, unit tests, performance)</li>
							<li class="fragment">Active community (6000+ stars, 1000+ citations, 3M+ downloads)</li>
							<li class="fragment">Fully documented</li>
						</ul>
						<aside class="notes">
							Maybe mention that we have different type of tests
							(run, unittest, performance)
						</aside>
					</section>

					<section>
						<h4>Performance Test Example</h4>
						<div class="row">
							<div class="col-xs-12 medium-text">
								<pre class="fragment"><code data-trim data-line-numbers="1-7|8-9|" class="python">
		# Training budget (cap the max number of iterations)
		N_STEPS = 1000

		def test_ppo():
			agent = PPO("MlpPolicy", "CartPole-v1").learn(N_STEPS)
			# Evaluate the trained agent
			episodic_return = evaluate_policy(agent, n_eval_episodes=20)
			# check that the performance is above a given threshold
			assert episodic_return > 90
								</code></pre>

							</div>
						</div>
					</section>
 
				</section>

				<section>
					<h4>SB3 + RL Zoo</h4>
					<img src="images/outlook/sb3_rl_zoo.png" alt="">
				</section>

				<section>
					<section>
						<div class="row">
							<div class="col-xs-12">
								<h4>RL Zoo: Reproducible Experiments</h4>
								<p class="medium-text">
									<a href="https://github.com/DLR-RM/rl-baselines3-zoo">
										https://github.com/DLR-RM/rl-baselines3-zoo
									</a>
								</p>
							</div>

						</div>
						<div class="row medium-text">
							<div class="col-xs-8">
								<ul>
									<li class="fragment">Training, loading, plotting, hyperparameter optimization</li>
									<li class="fragment">W&B and Huggingface integration</li>
									<li class="fragment">200+ trained models with tuned hyperparameters</li>
									<li class="fragment">
										<a href="https://wandb.ai/openrlbenchmark/sb3">
											OpenRL Benchmark
										</a>
									</li>
								</ul>
							</div>
							<div class="col-xs-4">
								<img src="https://github.com/DLR-RM/rl-baselines3-zoo/raw/master/images/car.jpg" class="shadow" alt="">
							</div>
						</div>

						<aside class="notes">
							RL Zoo: log everything that is needed to reproduce/compare automatically <br>
							Minimize potential mistake when running experiments <br>

						</aside>

					</section>
					<section>
						<h4>In practice</h4>
						<div class="row medium-text">
							<div class="col-xs-12">

							<pre class="fragment" style="width:100%"><code class="bash" data-line-numbers="1-5" data-trim>
								# Train an SAC agent on Pendulum using tuned hyperparameters,
								# evaluate the agent every 1k steps and save a checkpoint every 10k steps
								# Pass custom hyperparams to the algo/env
								python -m rl_zoo3.train --algo sac --env Pendulum-v1 --eval-freq 1000 \
								    --save-freq 10000 -params train_freq:2 --env-kwargs g:9.8
							</code></pre>

							<pre class="fragment" style="width:100%"><code class="bash" data-trim>
								sac/
								└── Pendulum-v1_1 # One folder per experiment
								    ├── 0.monitor.csv # episodic return
								    ├── best_model.zip # best model according to evaluation
								    ├── evaluations.npz # evaluation results
								    ├── Pendulum-v1
										│   ├── args.yml # custom cli arguments
										│   ├── config.yml # hyperparameters
								    │   └── vecnormalize.pkl # normalization
								    ├── Pendulum-v1.zip # final model
								    └── rl_model_10000_steps.zip # checkpoint

							</code></pre>
						</div>
					</div>

					<aside class="notes">
						Simple command in the terminal to launch an experiment
						and change some parameters

					</aside>

					</section>

					<section>
						<h4>Plotting</h4>
						<div class="row medium-text">
							<div class="col-xs-12">

							<pre style="width:100%"><code class="bash" data-line-numbers="|" data-trim>
								python -m rl_zoo3.cli all_plots -a sac -e HalfCheetah Ant -f logs/ -o sac_results
								python -m rl_zoo3.cli plot_from_file -i sac_results.pkl -latex -l SAC --rliable
							</code></pre>
						</div>
						<div class="col-xs-12">
							<img src="images/rl_zoo/rl_metrics.png" class="shadow" alt="">
						</div>
					</div>
					<aside class="notes">
						All experiments are formatted the same,
						makes it easy to plot/compare/follow best practices

					</aside>

					</section>

					<section>
						<h4>Open RL Benchmark</h4>
						<div class="col-xs-12">
							<a href="https://wandb.ai/openrlbenchmark/sb3/workspace?workspace=user-araffin" target="_blank">
								<img src="images/rl_zoo/wandb.png" style="max-width: 60%;" class="shadow" alt="">
							</a>
						</div>
						<aside class="notes">
							We benchmarked all algorithms on many tasks
							and the logs/results are available online
						</aside>

					</section>

				<section>
					<section>
						<h3>Implementing a New Algorithm</h3>
						<div class="r-stack">
							<img src="images/outlook/sb3_rl_zoo.png" alt="">
							<img class="fragment" src="images/outlook/sb3_new_algo.png" alt="">
						</div>
					</section>

					<section>
						<h5>1. Read the original paper several times</h5>
						<img class="shadow" style="max-width: 70%" src="images/bench/dqn.png" alt="">
						<img class="shadow" style="max-width: 80%" src="images/bench/dqn_appendix.png" alt="">
					</section>

					<section>
						<h5>2. Read existing implementations (if available)</h5>
						<img class="shadow" style="max-width: 90%" src="images/bench/td_lambda.png" alt="">
						<p class="small-text">
							<a href="https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/">
								The 37 Implementation Details of Proximal Policy Optimization
							</a>
						</p>

					</section>

					<section>
						<h5>3. Try to have some "sign of life" on toy problems</h5>
						<p class="medium-text">Iterate quickly!</p>

						<img class="shadow" style="max-width: 75%" src="images/bench/recurrent_ppo_bench.png" alt="">
						<img class="shadow" style="max-width: 20%" src="images/bench/pendulum.png" alt="">
						<p class="small-text">
							<a href="https://www.youtube.com/watch?v=8EcdaCk9KaQ">
								Nuts and Bolts of Deep RL Experimentation
							</a>
						</p>
					</section>

					<section>
						<h5>4. Step by step validation</h5>
						<p class="medium-text">Log useful values, <code class="medium-text">ipdb</code>, visualize</p>
						<img class="shadow" style="max-width: 75%" src="images/bench/broadcasting.png" alt="">
					</section>

					<section>
						<h5>5. Validation on known environments</h5>
						<p>Easy ➤ Medium ➤ Hard </p>
						<img class="shadow" style="max-width: 20%" src="images/bench/pendulum.png" alt="">
						<img class="shadow" style="max-width: 25%" src="images/bench/cheetah.png" alt="">
						<img class="shadow" style="max-width: 45%" src="images/bench/walker_hard.png" alt="">
					</section>

					<section>
						<h4>Some Examples</h4>
						<ul>
							<li class="fragment">SB2 PPO: broadcast error</li>
							<li class="fragment">SB3 A2C: TF RMSProp ≠ PyTorch RMSProp</li>
							<!-- <li class="fragment">SB3: proper timeout handling</li> -->
							<li class="fragment">SBX DQN: target network not updated</li>
						</ul>
						<p class="small-text fragment">
							More in the backup slides |
							<a href="https://twitter.com/araffin2/status/1331928661159657473" target="_blank">7 mistakes challenge</a>
						</p>
					</section>
					

				</section>

				<section>
					<h4>From complex codebase to minimal implementation</h4>
					<img src="images/outlook/outlook_minimal.png" style="max-width: 70%" alt="">
					<aside class="notes">
					 	a technique I'm using more and more recently,
						both for learning but also help for many other cases (i.e., thinking about how to simplify the codebase)
					</aside>

				</section>

				<section>
					<section>
						<h4>Minimal Implementations</h4>
						<div class="row">
							<div class="col-xs-7">
								<ul class="medium-text">
									<li class="fragment">Standalone / minimal dependencies</li>
									<li class="fragment">Reduce complexity</li>
									<li class="fragment">Easier to share/reproduce</li>
									<li class="fragment">Perfect for educational purposes (cleanRL)</li>
									<li class="fragment">Find bugs</li>
									<li class="fragment">Hard to maintain</li>
								</ul>

							</div>
							<div class="col-xs-5">
								<img src="images/minimal_impl.png" class="" alt="">
							</div>
						</div>
						<aside class="notes">
						 	do not have to look into 10 different to understand what is happening
						</aside>
					</section>

					

					<section>
						<h4>35 lines of code</h4>
						<div class="row">
							<div class="col-xs-12 small-text">
								<div>
									\[\begin{aligned}
									q^{\text{des}}_i(t) &amp;= \textcolor{#006400}{a_i} \cdot \sin(\theta_i(t) + \textcolor{#5f3dc4}{\varphi_i}) + \textcolor{#6d071a}{b_i} \\
									\dot{\theta_i}(t)  &amp;= \begin{cases}
										\textcolor{#0b7285}{\omega_\text{swing}}  &amp;\text{if $\sin(\theta_i(t) + \textcolor{#5f3dc4}{\varphi_i})) > 0$}\\
										\textcolor{#862e9c}{\omega_\text{stance}}  &amp;\text{otherwise.}
									\end{cases}

									\end{aligned} \]
								</div>
							</div>
							<div class="col-xs-12 fragment">
								<a href="https://gist.github.com/araffin/25159d668e9bad41bf31a595add22c27" target="_blank">
									<img src="images/minimal_swimmer.png" class="shadow" style="max-width: 50%" alt="">
								</a>
							</div>
						</div>
						<aside class="notes">
						 	Core idea: one oscillator per joint,
							share frequency between joints.
						</aside>
					</section>

					

				</section>

			

				<section>
					<h4>Conclusion</h4>
					<ul class="">
						<li class="fragment">Tips for reliable implementations</li>
						<li class="fragment">Reproducible experiments</li>
						<!-- <li class="fragment">Take it easy, RL is hard</li> -->
						<li class="fragment">Implementing a new algorithm</li>
						<li class="fragment">Minimal implementations to the rescue</li>
						<li class="fragment">Follow best practices</li>
					</ul>
				</section>

				<section>
					<h4>Questions?</h4>
				</section>
				<section>
					<h3>Backup slides</h3>
				</section>
				
			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script src="plugin/math/math.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				// Display the page number of the current slide
				slideNumber: true,

				// Add the current slide number to the URL hash so that reloading the
				// page/copying the URL will return you to the same slide
				hash: true,

				// Push each slide change to the browser history. Implies `hash: true`
				// history: false,

				// math: {
				// 	mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
				// 	config: 'TeX-AMS_HTML-full'  // See http://docs.mathjax.org/en/latest/config-files.html
				// 	// pass other options into `MathJax.Hub.Config()`
				// 	// TeX: { Macros: macros }
				// },

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath]
			});
		</script>
	</body>
</html>
